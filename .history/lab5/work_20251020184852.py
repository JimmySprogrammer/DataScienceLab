import pandas as pd
import os
import chardet
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

data_path = r"D:\DataScienceLab\lab5\data"

def detect_encoding(file_path):
    with open(file_path, "rb") as f:
        raw = f.read(10000)
    result = chardet.detect(raw)
    return result["encoding"] or "utf-8"

def load_all_data(path):
    datasets = {}
    print(f"ğŸ“‚ ä» {path} åŠ è½½æ•°æ®...\n")
    for file in os.listdir(path):
        if not file.endswith(".csv"):
            continue
        file_path = os.path.join(path, file)
        enc = detect_encoding(file_path)
        try:
            df = pd.read_csv(file_path, encoding=enc, sep=",", skiprows=1)
            df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]
            df["discipline"] = os.path.splitext(file)[0]
            datasets[file] = df
            print(f"âœ… æˆåŠŸè¯»å–: {file} ({len(df)} è¡Œ, ç¼–ç ={enc})")
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å– {file}ï¼Œé”™è¯¯ï¼š{e}")
    return datasets

def analyze_global_clusters(data_dict):
    merged = []
    for name, df in data_dict.items():
        possible_cols = [c for c in df.columns if "cites" in c or "documents" in c]
        if len(possible_cols) >= 2:
            num_cols = df[possible_cols].select_dtypes("number")
            if num_cols.shape[1] >= 2:
                df_num = num_cols.copy()
                df_num["discipline"] = name
                merged.append(df_num)
    if not merged:
        raise ValueError("âŒ æ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè¯·æ£€æŸ¥CSVç»“æ„")
    all_data = pd.concat(merged)
    all_data = all_data.dropna()
    X = all_data.select_dtypes("number").values
    X_scaled = StandardScaler().fit_transform(X)
    kmeans = KMeans(n_clusters=4, random_state=42)
    all_data["cluster"] = kmeans.fit_predict(X_scaled)
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(X_scaled)
    plt.scatter(reduced[:, 0], reduced[:, 1], c=all_data["cluster"])
    plt.title("å…¨çƒé«˜æ ¡èšç±»åˆ†å¸ƒ")
    plt.xlabel("PCA1")
    plt.ylabel("PCA2")
    plt.savefig("global_clusters.png")
    print("\nâœ… å·²ä¿å­˜å›¾åƒï¼šglobal_clusters.png")
    return all_data

def analyze_ecnu_profile(data_dict):
    ecnu = []
    for name, df in data_dict.items():
        col_inst = next((c for c in df.columns if "institut" in c), None)
        if col_inst is None:
            continue
        sub = df[df[col_inst].str.contains("East China Normal University", case=False, na=False)]
        if not sub.empty:
            sub["discipline"] = name
            ecnu.append(sub)
    if not ecnu:
        print("âš ï¸ æ•°æ®ä¸­æœªæ‰¾åˆ°åä¸œå¸ˆèŒƒå¤§å­¦è®°å½•")
        return
    ecnu_data = pd.concat(ecnu)
    print("\nğŸ“ åä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒï¼š")
    print(ecnu_data.head(10))
    if "cites/paper" in ecnu_data.columns:
        plt.barh(ecnu_data["discipline"], ecnu_data["cites/paper"])
        plt.title("åä¸œå¸ˆèŒƒå¤§å­¦å„å­¦ç§‘å½±å“åŠ›")
        plt.xlabel("Cites per Paper")
        plt.tight_layout()
        plt.savefig("ecnu_profile.png")
        print("âœ… å·²ä¿å­˜å›¾åƒï¼šecnu_profile.png")

def build_ranking_model(data_dict):
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import r2_score, mean_absolute_error

    all_df = []
    for name, df in data_dict.items():
        df = df.rename(columns=lambda x: x.strip().lower())
        if any("rank" in c for c in df.columns):
            num_cols = df.select_dtypes("number").copy()
            if num_cols.shape[1] >= 2:
                num_cols["discipline"] = name
                all_df.append(num_cols)
    if not all_df:
        print("âš ï¸ æ²¡æœ‰åŒ…å«æ’åçš„å­¦ç§‘æ•°æ®")
        return
    df = pd.concat(all_df)
    df = df.dropna()
    X = df.select_dtypes("number").iloc[:, :-1]
    y = df.select_dtypes("number").iloc[:, -1]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = LinearRegression().fit(X_train, y_train)
    preds = model.predict(X_test)
    print("\nğŸ“ˆ æ’åé¢„æµ‹æ¨¡å‹ç»“æœï¼š")
    print("RÂ² =", round(r2_score(y_test, preds), 3))
    print("MAE =", round(mean_absolute_error(y_test, preds), 3))

def main():
    data = load_all_data(data_path)
    print("\n=== ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ ===")
    merged = analyze_global_clusters(data)
    print("\n=== ç¬¬9é¢˜ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒ ===")
    analyze_ecnu_profile(data)
    print("\n=== ç¬¬10é¢˜ï¼šå­¦ç§‘æ’åé¢„æµ‹æ¨¡å‹ ===")
    build_ranking_model(data)

if __name__ == "__main__":
    main()
