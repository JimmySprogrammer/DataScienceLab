import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# ===========================
# åŸºç¡€è®¾ç½®
# ===========================
data_path = r"D:\DataScienceLab\lab5\data\merged_all.csv"
print(f"ğŸ“Š æ­£åœ¨è¯»å–åˆå¹¶æ•°æ®ï¼š{data_path}")

# è¯»å–æ•°æ®ï¼ˆè‡ªåŠ¨å°è¯•å¤šç§ç¼–ç ï¼‰
try:
    data = pd.read_csv(data_path, encoding="utf-8")
except:
    data = pd.read_csv(data_path, encoding="latin1")

# å»é™¤ä¹±ç åˆ—å
data.columns = [col.replace("Ã¯Â»Â¿", "").strip() for col in data.columns]
print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œæ€»è¡Œæ•°: {len(data)}")
print(f"ğŸ“‘ åˆ—: {list(data.columns)}")

# ===========================
# ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ
# ===========================
print("\n=== ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ ===")

features = ["Web of Science Documents", "Cites", "Cites/Paper", "Top Papers"]
df = data.dropna(subset=features)
X = df[features]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
df["Cluster"] = kmeans.fit_predict(X_scaled)

cluster_summary = df.groupby("Cluster")["Institutions"].count().to_frame()
print("âœ… æˆåŠŸå®Œæˆèšç±»åˆ†æï¼ˆå…±åˆ†5ç±»ï¼‰")
print(cluster_summary)

plt.figure(figsize=(7, 5))
cluster_summary["Institutions"].plot(kind="bar", color="black", edgecolor="gray")
plt.title("Global University Clusters")
plt.ylabel("Institution Count")
plt.xlabel("Cluster")
plt.tight_layout()
plt.savefig("cluster_plot.png")
print("ğŸ“ˆ å·²ç”Ÿæˆèšç±»åˆ†å¸ƒå›¾ cluster_plot.png")

# æŸ¥æ‰¾ä¸åä¸œå¸ˆèŒƒå¤§å­¦ç›¸ä¼¼çš„é«˜æ ¡
target = df[df["Institutions"].str.contains("EAST CHINA NORMAL UNIVERSITY", case=False, na=False)]
if not target.empty:
    cluster_id = target["Cluster"].iloc[0]
    similar = df[df["Cluster"] == cluster_id].sort_values("Cites", ascending=False).head(10)
    print(f"\nä¸ã€EAST CHINA NORMAL UNIVERSITYã€‘ç›¸ä¼¼çš„é«˜æ ¡åŒ…æ‹¬ï¼š")
    print(similar[["Institutions", "Cites", "Web of Science Documents"]])
else:
    print("âš ï¸ æ•°æ®ä¸­æœªæ‰¾åˆ° EAST CHINA NORMAL UNIVERSITY")

# ===========================
# ç¬¬9é¢˜ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒ
# ===========================
print("\n=== ç¬¬9é¢˜ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒ ===")

subject_col = "å­¦ç§‘" if "å­¦ç§‘" in data.columns else "Discipline"
ecnu = data[data["Institutions"].str.contains("EAST CHINA NORMAL UNIVERSITY", case=False, na=False)]

if not ecnu.empty:
    plt.figure(figsize=(10, 6))
    plt.bar(ecnu[subject_col], ecnu["Cites"], color="black", edgecolor="gray")
    plt.xticks(rotation=75, ha="right")
    plt.title("ECNU Subject Profile (Citations)")
    plt.ylabel("Citations")
    plt.tight_layout()
    plt.savefig("ecnu_profile.png")
    print("ğŸ“Š å·²ç”Ÿæˆåä¸œå¸ˆå¤§å­¦ç§‘ç”»åƒ ecnu_profile.png")
else:
    print("âš ï¸ æœªæ‰¾åˆ°åä¸œå¸ˆèŒƒå¤§å­¦ç›¸å…³æ•°æ®ï¼Œæ— æ³•ç»˜åˆ¶ç”»åƒã€‚")

# ===========================
# ç¬¬10é¢˜ï¼šæ’åé¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
# ===========================
print("\n=== ç¬¬10é¢˜ï¼šæ’åé¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ===")

df_model = data.dropna(subset=features + [subject_col]).copy()
X = df_model[["Web of Science Documents", "Cites/Paper", "Top Papers"]]
y = df_model["Cites"]

results = []

for subject, group in df_model.groupby(subject_col):
    if len(group) < 30:
        continue
    group_sorted = group.sort_values("Cites", ascending=False).reset_index(drop=True)
    n = len(group_sorted)
    train_end = int(n * 0.6)
    test_end = int(n * 0.8)

    train = group_sorted.iloc[:train_end]
    test = group_sorted.iloc[train_end:test_end]

    X_train = train[["Web of Science Documents", "Cites/Paper", "Top Papers"]]
    y_train = train["Cites"]
    X_test = test[["Web of Science Documents", "Cites/Paper", "Top Papers"]]
    y_test = test["Cites"]

    if len(X_train) < 5 or len(X_test) < 5:
        continue

    model = RandomForestRegressor(n_estimators=150, random_state=42)
    model.fit(X_train, y_train)
    r2 = model.score(X_test, y_test)

    preds = model.predict(X_test)
    group_result = test.copy()
    group_result["Predicted Cites"] = preds
    group_result["R2"] = r2
    results.append(group_result)

if results:
    final_results = pd.concat(results)
    print(f"ğŸ“ˆ ä½¿ç”¨å„å­¦ç§‘å‰60%è®­ç»ƒã€å20%æµ‹è¯•")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {int(len(df_model) * 0.6)}, æµ‹è¯•æ ·æœ¬æ•°: {int(len(df_model) * 0.2)}")
    print(f"æ¨¡å‹é¢„æµ‹ RÂ² å¹³å‡å¾—åˆ†: {np.mean([r for r in final_results['R2'] if not pd.isna(r)]):.4f}")

    print("\nç¤ºä¾‹é¢„æµ‹ç»“æœå‰10è¡Œï¼š")
    print(final_results[["Institutions", subject_col, "Cites", "Predicted Cites"]].head(10))
else:
    print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•å®Œæˆå»ºæ¨¡ã€‚")
