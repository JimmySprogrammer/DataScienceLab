# analysis.py
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

def main():
    data_path = r"D:\DataScienceLab\lab5\data\merged_all.csv"
    print(f"ğŸ“Š æ­£åœ¨è¯»å–åˆå¹¶æ•°æ®ï¼š{data_path}\n")

    df = pd.read_csv(data_path, encoding="latin1")

    df.columns = [c.strip() for c in df.columns]
    if 'Discipline' not in df.columns:
        print("âŒ ç¼ºå°‘ Discipline åˆ—ï¼Œè¯·ç¡®è®¤ merged_all.csv ç»“æ„æ˜¯å¦æ­£ç¡®")
        return

    df = df.dropna(subset=['Institutions', 'Countries/Regions', 'Web of Science Documents', 'Cites'])
    df = df[df['Web of Science Documents'].apply(lambda x: str(x).isdigit())]
    df['Web of Science Documents'] = df['Web of Science Documents'].astype(int)
    df['Cites'] = df['Cites'].astype(int)

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œæ€»è¡Œæ•°: {len(df)}")
    print(f"ğŸŒ åŒ…å«å›½å®¶æ•°é‡: {df['Countries/Regions'].nunique()}ï¼Œå­¦ç§‘æ•°é‡: {df['Discipline'].nunique()}\n")

    # ==== 1ï¸âƒ£ å›½å®¶å±‚é¢ç»Ÿè®¡ ====
    print("=== å„å›½è®ºæ–‡æ€»é‡ Top10 ===")
    country_papers = df.groupby("Countries/Regions")["Web of Science Documents"].sum().sort_values(ascending=False).head(10)
    print(country_papers, "\n")

    plt.figure(figsize=(10, 5))
    country_papers.plot(kind="bar", color="black", edgecolor="gray")
    plt.title("Top10 Countries by Publications")
    plt.xlabel("Country")
    plt.ylabel("Documents")
    plt.tight_layout()
    plt.show()

    # ==== 2ï¸âƒ£ å­¦ç§‘å±‚é¢ç»Ÿè®¡ ====
    print("=== å„å­¦ç§‘å¹³å‡å¼•æ–‡æ•° Top10 ===")
    discipline_cites = df.groupby("Discipline")["Cites"].mean().sort_values(ascending=False).head(10)
    print(discipline_cites, "\n")

    plt.figure(figsize=(10, 5))
    discipline_cites.plot(kind="barh", color="dimgray", edgecolor="black")
    plt.title("Top10 Disciplines by Average Citations")
    plt.xlabel("Average Cites")
    plt.ylabel("Discipline")
    plt.tight_layout()
    plt.show()

    # ==== 3ï¸âƒ£ å…¨çƒé«˜æ ¡ç§‘ç ”å½±å“åŠ›èšç±» ====
    print("=== KMeans èšç±»åˆ†æï¼ˆç§‘ç ”äº§å‡ºä¸å¼•ç”¨ï¼‰ ===")
    X = df[["Web of Science Documents", "Cites"]]
    X_scaled = StandardScaler().fit_transform(X)
    kmeans = KMeans(n_clusters=4, random_state=42)
    df["Cluster"] = kmeans.fit_predict(X_scaled)

    cluster_summary = df.groupby("Cluster")[["Web of Science Documents", "Cites"]].mean().round(1)
    print(cluster_summary, "\n")

    plt.figure(figsize=(7, 6))
    colors = ["black", "gray", "silver", "lightgray"]
    for i in range(4):
        cluster = df[df["Cluster"] == i]
        plt.scatter(cluster["Web of Science Documents"], cluster["Cites"], s=10, color=colors[i], label=f"Cluster {i}")
    plt.xlabel("Web of Science Documents")
    plt.ylabel("Cites")
    plt.title("Global University Clusters by Research Influence")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # ==== 4ï¸âƒ£ ç»“æœå¯¼å‡º ====
    output_path = r"D:\DataScienceLab\lab5\data\global_clusters.csv"
    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… èšç±»ç»“æœå·²ä¿å­˜: {output_path}")

if __name__ == "__main__":
    main()
