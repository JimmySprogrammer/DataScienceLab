import os
import pandas as pd
from glob import glob

def load_data(data_dir):
    csv_files = glob(os.path.join(data_dir, "*.csv"))
    data = {}

    for file in csv_files:
        df = None
        encodings_to_try = ["utf-8", "latin1", "gb18030"]

        for enc in encodings_to_try:
            try:
                df = pd.read_csv(file, encoding=enc)
                print(f"âœ… æˆåŠŸè¯»å–: {os.path.basename(file)} ({len(df)} è¡Œ, ç¼–ç ={enc})")
                break
            except Exception:
                continue

        if df is None:
            print(f"âŒ è¯»å–å¤±è´¥: {os.path.basename(file)}ï¼Œç¼–ç ä¸å…¼å®¹")
            continue

        # æ¸…æ´—åˆ—å
        df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

        # è‡ªåŠ¨å­—æ®µåŒ¹é…
        rename_map = {}
        for col in df.columns:
            if 'rank' in col:
                rename_map[col] = 'rank'
            elif 'cite' in col:
                rename_map[col] = 'cites_per_paper'
            elif 'doc' in col:
                rename_map[col] = 'documents'
            elif 'top' in col and 'paper' in col:
                rename_map[col] = 'top_papers'
            elif 'institution' in col:
                rename_map[col] = 'institution'
            elif 'country' in col:
                rename_map[col] = 'country'

        df = df.rename(columns=rename_map)

        data[os.path.splitext(os.path.basename(file))[0]] = df

    return data

def analyze_global_clusters(data):
    all_df = []
    for field, df in data.items():
        df['field'] = field
        required_cols = ['rank', 'cites_per_paper', 'documents', 'top_papers']

        missing = [c for c in required_cols if c not in df.columns]
        if missing:
            print(f"âš ï¸ {field} ç¼ºå°‘åˆ— {missing}ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
            continue

        df = df.dropna(subset=required_cols)
        all_df.append(df)

    if not all_df:
        raise ValueError("âŒ æ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè¯·æ£€æŸ¥CSVç»“æ„")

    merged = pd.concat(all_df, ignore_index=True)
    print(f"âœ… æˆåŠŸåˆå¹¶ {len(merged)} è¡Œæ•°æ®")
    return merged

def analyze_universities(merged):
    # åä¸œå¸ˆèŒƒå¤§å­¦çš„è¡¨ç°
    ecnus = merged[merged['institution'].str.contains("EAST CHINA NORMAL", case=False, na=False)]
    if not ecnus.empty:
        print("\n=== åä¸œå¸ˆèŒƒå¤§å­¦å„å­¦ç§‘æ’å ===")
        print(ecnus[['field', 'rank', 'cites_per_paper', 'documents', 'top_papers']])
    else:
        print("\nâš ï¸ æœªæ‰¾åˆ°åä¸œå¸ˆèŒƒå¤§å­¦ç›¸å…³è®°å½•")

    # ä¸­å›½ï¼ˆå¤§é™†ï¼‰å¤§å­¦è¡¨ç°
    cn_universities = merged[merged['country'].str.contains("CHINA", case=False, na=False)]
    print("\n=== ä¸­å›½å¤§é™†é«˜æ ¡æ€»ä½“è¡¨ç°ï¼ˆæŒ‰å­¦ç§‘å¹³å‡æ’åï¼‰ ===")
    summary_cn = cn_universities.groupby("field")["rank"].mean().reset_index().sort_values("rank")
    print(summary_cn)

    # å…¨çƒä¸åŒåŒºåŸŸè¡¨ç°
    print("\n=== å…¨çƒä¸åŒåŒºåŸŸè¡¨ç°ï¼ˆå¹³å‡æ’åï¼‰ ===")
    region_summary = merged.groupby("country")["rank"].mean().reset_index().sort_values("rank").head(10)
    print(region_summary)

def main():
    data_dir = r"D:\DataScienceLab\lab5\data"
    print(f"ğŸ“‚ æ­£åœ¨ä» {data_dir} åŠ è½½æ•°æ®...\n")

    data = load_data(data_dir)
    print("\n=== ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ ===")
    merged = analyze_global_clusters(data)
    analyze_universities(merged)

if __name__ == "__main__":
    main()
