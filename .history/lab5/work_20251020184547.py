import os
import pandas as pd
from glob import glob

def load_data(data_dir):
    csv_files = glob(os.path.join(data_dir, "*.csv"))
    data = {}

    for file in csv_files:
        df = None
        encodings = ["utf-8", "latin1", "gb18030"]
        for enc in encodings:
            try:
                df = pd.read_csv(file, encoding=enc, header=None)
                break
            except Exception:
                continue

        if df is None:
            print(f"âŒ æ— æ³•è¯»å– {os.path.basename(file)}ï¼Œè·³è¿‡ã€‚")
            continue

        # æ‰¾å‡ºç¬¬ä¸€è¡Œä¸­åŒ…å«å…³é”®å­— "Institutions" çš„è¡Œï¼Œä½œä¸º header
        header_row = None
        for i in range(min(10, len(df))):
            if df.iloc[i].astype(str).str.contains("Institution", case=False, na=False).any():
                header_row = i
                break

        if header_row is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°è¡¨å¤´: {os.path.basename(file)}ï¼Œè·³è¿‡ã€‚")
            continue

        df.columns = df.iloc[header_row]
        df = df.drop(index=range(header_row + 1))
        df = df.reset_index(drop=True)

        df.columns = [str(c).strip().lower().replace(" ", "_") for c in df.columns]

        rename_map = {}
        for c in df.columns:
            if "institution" in c:
                rename_map[c] = "institution"
            elif "country" in c:
                rename_map[c] = "country"
            elif "document" in c:
                rename_map[c] = "documents"
            elif "cite/paper" in c or "cites_per_paper" in c:
                rename_map[c] = "cites_per_paper"
            elif "top" in c and "paper" in c:
                rename_map[c] = "top_papers"
            elif "rank" in c:
                rename_map[c] = "rank"

        df = df.rename(columns=rename_map)

        # ä¿ç•™å…³é”®åˆ—
        keep_cols = ["institution", "country", "documents", "cites_per_paper", "top_papers"]
        df = df[[c for c in keep_cols if c in df.columns]]
        df["field"] = os.path.splitext(os.path.basename(file))[0]
        data[os.path.basename(file)] = df

        print(f"âœ… æˆåŠŸè§£æ: {os.path.basename(file)} ({len(df)} è¡Œ)")

    return data

def analyze_global_clusters(data):
    all_df = []
    for name, df in data.items():
        if len(df) < 10:
            continue
        all_df.append(df)
    if not all_df:
        raise ValueError("âŒ æ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè¯·æ£€æŸ¥CSVç»“æ„")

    merged = pd.concat(all_df, ignore_index=True)
    print(f"âœ… æˆåŠŸåˆå¹¶ {len(merged)} è¡Œæ•°æ®")
    return merged

def analyze_universities(merged):
    ecnus = merged[merged["institution"].str.contains("EAST CHINA NORMAL", case=False, na=False)]
    if not ecnus.empty:
        print("\n=== åä¸œå¸ˆèŒƒå¤§å­¦å„å­¦ç§‘è¡¨ç° ===")
        print(ecnus[["field", "documents", "cites_per_paper", "top_papers"]])
    else:
        print("\nâš ï¸ æœªæ‰¾åˆ°åä¸œå¸ˆèŒƒå¤§å­¦è®°å½•")

    cn_universities = merged[merged["country"].str.contains("CHINA", case=False, na=False)]
    print("\n=== ä¸­å›½é«˜æ ¡æ€»ä½“è¡¨ç°ï¼ˆæŒ‰å­¦ç§‘å¹³å‡å¼•æ–‡æ•°ï¼‰ ===")
    summary_cn = cn_universities.groupby("field")["cites_per_paper"].mean().reset_index().sort_values("cites_per_paper", ascending=False)
    print(summary_cn)

    print("\n=== å…¨çƒåœ°åŒºè¡¨ç°ï¼ˆå¹³å‡å¼•æ–‡æ•°Top10ï¼‰ ===")
    region_summary = merged.groupby("country")["cites_per_paper"].mean().reset_index().sort_values("cites_per_paper", ascending=False).head(10)
    print(region_summary)

def main():
    data_dir = r"D:\DataScienceLab\lab5\data"
    print(f"ğŸ“‚ ä» {data_dir} åŠ è½½æ•°æ®...\n")
    data = load_data(data_dir)
    print("\n=== ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ ===")
    merged = analyze_global_clusters(data)
    analyze_universities(merged)

if __name__ == "__main__":
    main()
