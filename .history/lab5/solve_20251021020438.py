import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
import numpy as np
import warnings
import os

warnings.filterwarnings("ignore")
os.environ["LOKY_MAX_CPU_COUNT"] = "8"

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸ“Š æ­£åœ¨è¯»å–åˆå¹¶æ•°æ®ï¼šD:\\DataScienceLab\\lab5\\data\\merged_all.csv")

df = pd.read_csv("D:\\DataScienceLab\\lab5\\data\\merged_all.csv")
print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œæ€»è¡Œæ•°: {len(df)}")
print("ğŸ“‘ åˆ—:", list(df.columns))

# ========== ç¬¬8é¢˜ ==========
print("\n=== ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ ===")

features = ["Web of Science Documents", "Cites", "Cites/Paper", "Top Papers"]
df_clean = df.dropna(subset=features).copy()
X_scaled = StandardScaler().fit_transform(df_clean[features])

kmeans = KMeans(n_clusters=5, random_state=42)
df_clean.loc[:, "Cluster"] = kmeans.fit_predict(X_scaled)

cluster_summary = df_clean.groupby("Cluster")["Institutions"].count().to_frame("count")
print("âœ… æˆåŠŸå®Œæˆèšç±»åˆ†æï¼ˆå…±åˆ†5ç±»ï¼‰\n", cluster_summary)

sns.countplot(x="Cluster", data=df_clean, palette="viridis")
plt.title("å…¨çƒé«˜æ ¡èšç±»åˆ†å¸ƒ")
plt.xlabel("Cluster")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig("cluster_plot.png", dpi=300)
print("ğŸ“ˆ å·²ç”Ÿæˆèšç±»åˆ†å¸ƒå›¾ cluster_plot.png")

target_uni = "EAST CHINA NORMAL UNIVERSITY"
target_cluster = df_clean[df_clean["Institutions"].str.contains(target_uni, case=False, na=False)]["Cluster"].iloc[0]
similar_universities = df_clean[df_clean["Cluster"] == target_cluster].nlargest(10, "Cites")[["Institutions", "Cites", "Web of Science Documents"]]
print(f"\nä¸ã€{target_uni}ã€‘ç›¸ä¼¼çš„é«˜æ ¡åŒ…æ‹¬ï¼š\n", similar_universities)

# ========== ç¬¬9é¢˜ ==========
print("\n=== ç¬¬9é¢˜ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒ ===")

ecnu_data = df[df["Institutions"].str.contains(target_uni, case=False, na=False)]
discipline_stats = ecnu_data.groupby("Discipline")["Cites"].sum().sort_values(ascending=False).head(10)

plt.figure(figsize=(10, 6))
sns.barplot(x=discipline_stats.values, y=discipline_stats.index, palette="coolwarm")
plt.title("åä¸œå¸ˆå¤§å­¦ç§‘ç”»åƒï¼ˆæŒ‰æ€»è¢«å¼•é¢‘æ¬¡ï¼‰")
plt.xlabel("æ€»è¢«å¼•é¢‘æ¬¡")
plt.ylabel("å­¦ç§‘")
plt.tight_layout()
plt.savefig("ecnu_profile.png", dpi=300)
print("ğŸ“Š å·²ç”Ÿæˆåä¸œå¸ˆå¤§å­¦ç§‘ç”»åƒ ecnu_profile.png")

# ========== ç¬¬10é¢˜ ==========
print("\n=== ç¬¬10é¢˜ï¼šæ’åé¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ===")

model_data = df.dropna(subset=["Cites", "Web of Science Documents", "Cites/Paper", "Top Papers"])
X = model_data[["Web of Science Documents", "Cites/Paper", "Top Papers"]]
y = model_data["Cites"]

train, test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"ğŸ“ˆ ä½¿ç”¨å„å­¦ç§‘å‰60%è®­ç»ƒã€å20%æµ‹è¯•")

model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
model.fit(train, y_train)

scores = cross_val_score(model, X, y, cv=5, scoring="r2")
print(f"æ¨¡å‹é¢„æµ‹ RÂ² å¹³å‡å¾—åˆ†: {scores.mean():.4f}")

y_pred = model.predict(test)
test = test.copy()
test.loc[:, "Predicted Cites"] = y_pred
test.loc[:, "Institutions"] = model_data["Institutions"].iloc[test.index]
test.loc[:, "Discipline"] = model_data["Discipline"].iloc[test.index]

print("\nç¤ºä¾‹é¢„æµ‹ç»“æœå‰10è¡Œï¼š")
print(test.head(10)[["Institutions", "Discipline", "Cites", "Predicted Cites"]])
