import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score

# === è·¯å¾„é…ç½® ===
path = r"D:\DataScienceLab\lab5\data\merged_all.csv"

print(f"ğŸ“Š æ­£åœ¨è¯»å–åˆå¹¶æ•°æ®ï¼š{path}")
data = pd.read_csv(path, encoding='utf-8')
print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œæ€»è¡Œæ•°: {len(data)}")
print(f"ğŸ“‘ åˆ—: {list(data.columns)}")

# ç¡®è®¤åˆ—åæ˜ å°„ï¼ˆé˜²æ­¢æœ‰çš„ç‰ˆæœ¬å« Disciplineï¼‰
if "Discipline" in data.columns:
    data.rename(columns={"Discipline": "å­¦ç§‘"}, inplace=True)

# ===== ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ =====
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler

print("\n=== ç¬¬8é¢˜ï¼šå…¨çƒé«˜æ ¡åˆ†ç±»åˆ†æ ===")
df = data.copy()
features = ["Web of Science Documents", "Cites", "Cites/Paper", "Top Papers"]
df = df.dropna(subset=features)

X = df[features]
X_scaled = MinMaxScaler().fit_transform(X)
kmeans = KMeans(n_clusters=5, random_state=42)
df["Cluster"] = kmeans.fit_predict(X_scaled)

print("âœ… æˆåŠŸå®Œæˆèšç±»åˆ†æï¼ˆå…±åˆ†5ç±»ï¼‰")
print(df["Cluster"].value_counts().to_frame())

plt.figure(figsize=(8, 6))
plt.scatter(df["Web of Science Documents"], df["Cites"], c=df["Cluster"], cmap="tab10", s=10)
plt.title("Global University Clusters")
plt.xlabel("Web of Science Documents")
plt.ylabel("Cites")
plt.savefig("cluster_plot.png", dpi=300)
plt.close()
print("ğŸ“ˆ å·²ç”Ÿæˆèšç±»åˆ†å¸ƒå›¾ cluster_plot.png")

# ä¸ECNUç›¸ä¼¼çš„é™¢æ ¡
target = "EAST CHINA NORMAL UNIVERSITY"
if target in df["Institutions"].values:
    target_cluster = df.loc[df["Institutions"] == target, "Cluster"].iloc[0]
    similar = df[df["Cluster"] == target_cluster].sort_values("Cites", ascending=False).head(10)
    print(f"\nä¸ã€{target}ã€‘ç›¸ä¼¼çš„é«˜æ ¡åŒ…æ‹¬ï¼š")
    print(similar[["Institutions", "Cites", "Web of Science Documents"]])
else:
    print(f"\nâš ï¸ æœªæ‰¾åˆ°ç›®æ ‡é™¢æ ¡ {target}")

# ===== ç¬¬9é¢˜ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒ =====
print("\n=== ç¬¬9é¢˜ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒ ===")
ecnu = data[data["Institutions"].str.contains("EAST CHINA NORMAL UNIVERSITY", case=False, na=False)]
profile = ecnu.groupby("å­¦ç§‘")[["Cites", "Web of Science Documents", "Top Papers"]].sum().sort_values("Cites", ascending=False)
profile.plot(kind="bar", figsize=(10, 6))
plt.title("åä¸œå¸ˆèŒƒå¤§å­¦å­¦ç§‘ç”»åƒ")
plt.ylabel("æ•°é‡")
plt.tight_layout()
plt.savefig("ecnu_profile.png", dpi=300)
plt.close()
print("ğŸ“Š å·²ç”Ÿæˆåä¸œå¸ˆå¤§å­¦ç§‘ç”»åƒ ecnu_profile.png")

# ===== ç¬¬10é¢˜ï¼šæ’åé¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====
print("\n=== ç¬¬10é¢˜ï¼šæ’åé¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ===")
print("ğŸ“ˆ ä½¿ç”¨å„å­¦ç§‘å‰60%è®­ç»ƒã€å20%æµ‹è¯•")

all_train, all_test = [], []
r2_scores = []

for subject, group in data.groupby("å­¦ç§‘"):
    group = group.dropna(subset=["Cites", "Web of Science Documents"])
    if len(group) < 10:
        continue

    group = group.sort_values("Cites", ascending=False).reset_index(drop=True)
    n = len(group)
    train_end = int(n * 0.6)
    test_start = int(n * 0.8)

    train = group.iloc[:train_end]
    test = group.iloc[test_start:]

    features = ["Web of Science Documents", "Cites/Paper", "Top Papers"]
    X_train = train[features].fillna(0)
    X_test = test[features].fillna(0)
    y_train = train["Cites"]
    y_test = test["Cites"]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    test["Predicted Cites"] = y_pred
    all_test.append(test)
    score = r2_score(y_test, y_pred)
    r2_scores.append(score)

final_results = pd.concat(all_test)
mean_r2 = np.mean(r2_scores)

print(f"è®­ç»ƒæ ·æœ¬æ•°: {sum(len(g) for g in data.groupby('å­¦ç§‘')) * 0.6:.0f}, æµ‹è¯•æ ·æœ¬æ•°: {len(final_results)}")
print(f"æ¨¡å‹é¢„æµ‹ RÂ² å¹³å‡å¾—åˆ†: {mean_r2:.4f}")

print("\nç¤ºä¾‹é¢„æµ‹ç»“æœå‰10è¡Œï¼š")
print(final_results[["Institutions", "å­¦ç§‘", "Cites", "Predicted Cites"]].head(10))

# ä¿å­˜é¢„æµ‹ç»“æœ
final_results.to_csv("prediction_results.csv", index=False, encoding="utf-8-sig")
print("\nğŸ’¾ å·²ä¿å­˜é¢„æµ‹ç»“æœè‡³ prediction_results.csv")
