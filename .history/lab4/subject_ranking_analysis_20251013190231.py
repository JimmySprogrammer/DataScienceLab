{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa593c",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subject\n\u001b[0;32m     20\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m---> 22\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_data, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, csv_files)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\19691\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\19691\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\19691\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  Subject Ranking Analysis\n",
    "#  Author: ChatGPT\n",
    "# ================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ================================================================\n",
    "# 1. Load all CSV files\n",
    "# ================================================================\n",
    "data_path = r\"D:\\DataScienceLab\\lab4\\data\"  # âœ… ä½ çš„CSVæ–‡ä»¶ç›®å½•\n",
    "csv_files = glob.glob(os.path.join(data_path, '*.csv'))\n",
    "\n",
    "print(\"ğŸ” Found CSV files:\")\n",
    "print(csv_files, \"\\n\")\n",
    "\n",
    "all_data = []\n",
    "for file in csv_files:\n",
    "    subject = os.path.splitext(os.path.basename(file))[0]\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file, encoding='gbk')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {file}: {e}\")\n",
    "        continue\n",
    "    df['Subject'] = subject\n",
    "    all_data.append(df)\n",
    "    print(f\"âœ… Loaded {os.path.basename(file)}, shape = {df.shape}\")\n",
    "\n",
    "if not all_data:\n",
    "    raise ValueError(\"âš ï¸ No CSV files loaded. Please check your path or encoding.\")\n",
    "else:\n",
    "    data = pd.concat(all_data, ignore_index=True)\n",
    "    print(\"ğŸ‰ Data successfully combined:\", data.shape)\n",
    "    print(\"ğŸ“Š Preview:\")\n",
    "    print(data.head())\n",
    "\n",
    "# ================================================================\n",
    "# 2. Create SQLite database\n",
    "# ================================================================\n",
    "conn = sqlite3.connect('university_rankings.db')\n",
    "data.to_sql('raw_rankings', conn, if_exists='replace', index=False)\n",
    "\n",
    "# ================================================================\n",
    "# 3. Normalize schema\n",
    "# ================================================================\n",
    "# æ ¹æ®ä½ çš„CSVç»“æ„ï¼Œè¿™é‡Œå‡è®¾è‡³å°‘æœ‰ä»¥ä¸‹å­—æ®µï¼š\n",
    "# Institution, Country, Region, Rank, Score\n",
    "# å¦‚æœåˆ—åä¸å®Œå…¨ä¸€è‡´ï¼Œå¯åœ¨æ­¤å¤„é‡å‘½å\n",
    "\n",
    "cols = [c.lower() for c in data.columns]\n",
    "data.columns = cols\n",
    "\n",
    "# è‡ªåŠ¨æ£€æµ‹åŒ¹é…åˆ—\n",
    "inst_col = [c for c in cols if 'institution' in c or 'university' in c][0]\n",
    "country_col = [c for c in cols if 'country' in c][0]\n",
    "region_col = [c for c in cols if 'region' in c or 'area' in c][0]\n",
    "rank_col = [c for c in cols if 'rank' in c][0]\n",
    "score_col = [c for c in cols if 'score' in c or 'points' in c][0]\n",
    "\n",
    "universities = data[[inst_col, country_col, region_col]].drop_duplicates().reset_index(drop=True)\n",
    "universities.columns = ['Institution', 'Country', 'Region']\n",
    "universities['UniversityID'] = universities.index + 1\n",
    "\n",
    "subjects = pd.DataFrame({'Subject': data['subject'].unique()})\n",
    "subjects['SubjectID'] = subjects.index + 1\n",
    "\n",
    "rankings = data.merge(universities, left_on=[inst_col, country_col, region_col],\n",
    "                      right_on=['Institution', 'Country', 'Region'], how='left')\n",
    "rankings = rankings.merge(subjects, on='Subject', how='left')\n",
    "rankings_table = rankings[['UniversityID', 'SubjectID', rank_col, score_col]]\n",
    "rankings_table.columns = ['UniversityID', 'SubjectID', 'Rank', 'Score']\n",
    "\n",
    "universities.to_sql('universities', conn, if_exists='replace', index=False)\n",
    "subjects.to_sql('subjects', conn, if_exists='replace', index=False)\n",
    "rankings_table.to_sql('rankings', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"\\nâœ… Database created successfully with normalized schema.\")\n",
    "\n",
    "# ================================================================\n",
    "# 4. Queries\n",
    "# ================================================================\n",
    "print(\"\\n==============================\")\n",
    "print(\"åä¸œå¸ˆèŒƒå¤§å­¦å„å­¦ç§‘æ’å\")\n",
    "print(\"==============================\")\n",
    "\n",
    "query_ecnu = \"\"\"\n",
    "SELECT s.Subject, r.Rank, r.Score\n",
    "FROM rankings r\n",
    "JOIN universities u ON r.UniversityID = u.UniversityID\n",
    "JOIN subjects s ON r.SubjectID = s.SubjectID\n",
    "WHERE u.Institution LIKE '%East China Normal University%'\n",
    "ORDER BY r.Rank ASC;\n",
    "\"\"\"\n",
    "ecnu_rank = pd.read_sql(query_ecnu, conn)\n",
    "print(ecnu_rank)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"ä¸­å›½ï¼ˆå¤§é™†åœ°åŒºï¼‰å¤§å­¦å„å­¦ç§‘è¡¨ç°\")\n",
    "print(\"==============================\")\n",
    "\n",
    "query_china = \"\"\"\n",
    "SELECT s.Subject,\n",
    "       COUNT(*) AS Num_Universities,\n",
    "       AVG(r.Rank) AS Avg_Rank,\n",
    "       AVG(r.Score) AS Avg_Score\n",
    "FROM rankings r\n",
    "JOIN universities u ON r.UniversityID = u.UniversityID\n",
    "JOIN subjects s ON r.SubjectID = s.SubjectID\n",
    "WHERE u.Country = 'China'\n",
    "GROUP BY s.Subject\n",
    "ORDER BY Avg_Score DESC;\n",
    "\"\"\"\n",
    "china_perf = pd.read_sql(query_china, conn)\n",
    "print(china_perf)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"å…¨çƒä¸åŒåŒºåŸŸå„å­¦ç§‘è¡¨ç°\")\n",
    "print(\"==============================\")\n",
    "\n",
    "query_region = \"\"\"\n",
    "SELECT u.Region,\n",
    "       s.Subject,\n",
    "       COUNT(*) AS Num_Universities,\n",
    "       AVG(r.Score) AS Avg_Score,\n",
    "       AVG(r.Rank) AS Avg_Rank\n",
    "FROM rankings r\n",
    "JOIN universities u ON r.UniversityID = u.UniversityID\n",
    "JOIN subjects s ON r.SubjectID = s.SubjectID\n",
    "GROUP BY u.Region, s.Subject\n",
    "ORDER BY s.Subject, Avg_Score DESC;\n",
    "\"\"\"\n",
    "region_perf = pd.read_sql(query_region, conn)\n",
    "print(region_perf)\n",
    "\n",
    "# ================================================================\n",
    "# 5. Close connection\n",
    "# ================================================================\n",
    "conn.close()\n",
    "print(\"\\nâœ… Analysis completed successfully!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
