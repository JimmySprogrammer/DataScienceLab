import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import matplotlib.pyplot as plt

# è·¯å¾„è®¾ç½®
data_path = r"D:\DataScienceLab\lab5\data\merged_all.csv"
output_dir = r"D:\DataScienceLab\lab7"
os.makedirs(output_dir, exist_ok=True)

print("ğŸ“Š è¯»å–æ•°æ®:", data_path)
df = pd.read_csv(data_path)

# æ¸…æ´—æ•°æ®
df = df.dropna(subset=["Institutions", "Cites", "Web of Science Documents"])
df = df[df["Cites"] > 0]
print("æ•°æ®è¡Œæ•°:", len(df))

# å°†å­¦ç§‘è½¬æ¢ä¸ºæ ‡ç­¾
df["Discipline"] = df["Discipline"].astype(str)
disciplines = df["Discipline"].unique()

# ==========================
# ğŸ”¹ 1. èšç±»åˆ†æï¼ˆä¸ºæ¨¡å‹æä¾›è¾…åŠ©ç‰¹å¾ï¼‰
# ==========================
print("\nğŸ” æ‰§è¡ŒKMeansèšç±» (k=8)...")
num_features = ["Cites", "Web of Science Documents", "Cites/Paper", "Top Papers"]
X_cluster = df[num_features].fillna(0)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

kmeans = KMeans(n_clusters=8, random_state=42)
df["ClusterLabel"] = kmeans.fit_predict(X_scaled)

cluster_csv = os.path.join(output_dir, "clustering_results_v2.csv")
df.to_csv(cluster_csv, index=False)
print(f"âœ… èšç±»ç»“æœå·²ä¿å­˜åˆ°: {cluster_csv}")

# å¯è§†åŒ–èšç±»
plt.figure(figsize=(8,6))
plt.scatter(X_scaled[:,0], X_scaled[:,1], c=df["ClusterLabel"], cmap="tab10", s=10)
plt.title("University Clusters based on ESI Data")
plt.xlabel("Cites (scaled)")
plt.ylabel("Documents (scaled)")
plt.savefig(os.path.join(output_dir, "cluster_visualization_v2.png"))
plt.close()
print("âœ… èšç±»å¯è§†åŒ–å›¾å·²ä¿å­˜")

# ==========================
# ğŸ”¹ 2. æ·±åº¦å­¦ä¹ å­¦ç§‘æ’åé¢„æµ‹æ¨¡å‹
# ==========================
print("\nğŸ« å¼€å§‹è®­ç»ƒæ”¹è¿›ç‰ˆæ·±åº¦å­¦ä¹ æ¨¡å‹...")

results = []
for disc in disciplines:
    sub = df[df["Discipline"] == disc]
    if len(sub) < 300:
        continue

    X = sub[["Web of Science Documents", "Cites", "Cites/Paper", "Top Papers", "ClusterLabel"]].fillna(0)
    y = np.arange(len(sub))  # æ¨¡æ‹Ÿæ’åï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler2 = StandardScaler()
    X_train_scaled = scaler2.fit_transform(X_train)
    X_test_scaled = scaler2.transform(X_test)

    # æ„å»ºæ”¹è¿›æ¨¡å‹
    model = Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(1)
    ])

    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])
    model.fit(X_train_scaled, y_train, epochs=25, batch_size=16, verbose=0)

    preds = model.predict(X_test_scaled).flatten()
    mse = mean_squared_error(y_test, preds)
    mape = mean_absolute_percentage_error(y_test, preds)

    results.append((disc, len(sub), round(mse, 3), round(mape, 4)))
    print(f"[{disc}] n={len(sub)} MSE={mse:.2f} MAPE={mape:.3f}")

# ä¿å­˜é¢„æµ‹ç»“æœ
results_df = pd.DataFrame(results, columns=["Discipline", "Samples", "MSE", "MAPE"])
results_path = os.path.join(output_dir, "ranking_predictions_v2.csv")
results_df.to_csv(results_path, index=False)
print(f"\nâœ… æ”¹è¿›ç‰ˆé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

# ==========================
# ğŸ”¹ 3. è¾“å‡ºæ€»ç»“ç»“æœ
# ==========================
summary = results_df.sort_values("MSE").reset_index(drop=True)
print("\nğŸ æ¨¡å‹è¡¨ç°æœ€ä½³çš„å‰5ä¸ªå­¦ç§‘:")
print(summary.head())

print(f"\nğŸ¯ å…¨éƒ¨å®Œæˆï¼ç»“æœæ–‡ä»¶ä½äº: {output_dir}")
