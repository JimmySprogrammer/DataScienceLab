# tasks11_12.py
# -*- coding: utf-8 -*-
"""
ä»»åŠ¡11 & 12 è„šæœ¬
- æ•°æ®ä½ç½®: D:\DataScienceLab\lab5\data\merged_all.csv
- è¾“å‡ºï¼š
    - models/ (ä¿å­˜è®­ç»ƒçš„æ¨¡å‹æƒé‡)
    - predictions_{discipline}.csv (æ¯ä¸ªå­¦ç§‘çš„æµ‹è¯•é›†é¢„æµ‹ä¸è¯„ä¼°)
    - dl_summary.csv (æ±‡æ€»æ¯ä¸ªå­¦ç§‘ MSE, MAPE)
    - cluster_plot.png, ecnu_cluster_similars.csv (èšç±»ç»“æœä¸ä¸ECNUç›¸ä¼¼é«˜æ ¡)
    - cluster_analysis.csv (ç°‡å†…ç‰¹å¾å‡å€¼ï¼Œç”¨äºåŸå› åˆ†æ)
"""
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# ---------------------------
# é…ç½®
# ---------------------------
DATA_PATH = r"D:\DataScienceLab\lab5\data\merged_all.csv"
OUT_DIR = r"D:\DataScienceLab\lab5\outputs_tasks11_12"
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.join(OUT_DIR, "models"), exist_ok=True)

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# ---------------------------
# è¯»å–ä¸æ¸…æ´—æ•°æ®
# ---------------------------
print("ğŸ“Š è¯»å–æ•°æ®:", DATA_PATH)
df = pd.read_csv(DATA_PATH, encoding="utf-8")
print("åŸå§‹è¡Œæ•°:", len(df))
print("åˆ—:", list(df.columns))

# ä¿ç•™å¿…è¦åˆ—å¹¶è½¬ä¸ºæ•°å€¼ç±»å‹
cols_needed = ["Institutions", "Discipline", "Web of Science Documents", "Cites", "Cites/Paper", "Top Papers"]
for c in cols_needed:
    if c not in df.columns:
        raise SystemExit(f"ç¼ºå°‘åˆ—: {c}ï¼Œè¯·æ£€æŸ¥ merged_all.csv åˆ—åã€‚")

df = df[cols_needed].copy()
df["Web of Science Documents"] = pd.to_numeric(df["Web of Science Documents"], errors="coerce").fillna(0)
df["Cites"] = pd.to_numeric(df["Cites"], errors="coerce").fillna(0)
df["Cites/Paper"] = pd.to_numeric(df["Cites/Paper"], errors="coerce").fillna(0)
df["Top Papers"] = pd.to_numeric(df["Top Papers"], errors="coerce").fillna(0)

# ä¸ºæ¯ä¸ªå­¦ç§‘è®¡ç®—â€œRankâ€ï¼ˆåŸºäº Cites é™åºï¼‰
df["Rank"] = df.groupby("Discipline")["Cites"].rank(method="first", ascending=False)
# å°† Rank è½¬ä¸ºæ•´æ•°ï¼ˆæ’åä»1å¼€å§‹ï¼‰
df["Rank"] = df["Rank"].astype(int)

# è¿‡æ»¤æ‰æ ·æœ¬æ•°éå¸¸å°‘çš„å­¦ç§‘ï¼ˆä¾‹å¦‚ < 30ï¼‰ï¼Œä½†ä»ä¼šè®°å½•
discipline_counts = df["Discipline"].value_counts()
print("å­¦ç§‘æ ·æœ¬æ•°ï¼ˆå‰10ï¼‰ï¼š\n", discipline_counts.head(10))

# ---------------------------
# ç¬¬11é¢˜ï¼šæŒ‰å­¦ç§‘è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹æ’å
# ç­–ç•¥ï¼šå¯¹æ¯ä¸ªå­¦ç§‘å•ç‹¬å»ºæ¨¡
#   - æŒ‰ Cites æ’åºï¼ˆä¸ä¹‹å‰ä¸€è‡´ï¼‰ï¼Œå–å‰60%è®­ç»ƒã€å20%æµ‹è¯•ï¼ˆä¸­é—´20%å¿½ç•¥ï¼‰
#   - ç‰¹å¾ï¼š ["Web of Science Documents", "Cites/Paper", "Top Papers"]ï¼ˆå¯æ‰©å±•ï¼‰
#   - æ¨¡å‹ï¼šå°å‹å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œè¾“å‡ºé¢„æµ‹ Rankï¼ˆå›å½’ï¼‰
#   - è¯„ä¼°æŒ‡æ ‡ï¼šMSE, MAPE
# ---------------------------

features = ["Web of Science Documents", "Cites/Paper", "Top Papers"]
target = "Rank"

summary_rows = []
all_test_results = []

# Keras é»˜è®¤æ—¥å¿—å¤ªå¤šï¼Œé™ä½ verbosity
tf.get_logger().setLevel('ERROR')

for disc, group in df.groupby("Discipline"):
    n = len(group)
    if n < 30:
        print(f"è·³è¿‡å­¦ç§‘ï¼ˆæ ·æœ¬å¤ªå°‘ï¼‰: {disc} (n={n})")
        continue

    # æŒ‰ Cites æ’åºï¼Œç¡®ä¿åˆ’åˆ†å«ä¹‰ä¸€è‡´ï¼ˆè¶Šé«˜çš„ Cites => æ›´å¥½æ’åï¼‰
    group_sorted = group.sort_values(by="Cites", ascending=False).reset_index(drop=True)

    train_end = int(0.6 * n)
    test_start = int(0.8 * n)

    train_df = group_sorted.iloc[:train_end].reset_index(drop=True)
    test_df = group_sorted.iloc[test_start:].reset_index(drop=True)

    X_train = train_df[features].values
    y_train = train_df[target].values.astype(float)
    X_test = test_df[features].values
    y_test = test_df[target].values.astype(float)

    # æ ‡å‡†åŒ–ï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # æ„å»ºå°å‹ MLP
    # è¾“å…¥å±‚ -> Dense(64) -> Dense(32) -> è¾“å‡ºï¼ˆçº¿æ€§ï¼‰
    model = keras.Sequential([
        layers.Input(shape=(len(features),)),
        layers.Dense(64, activation="relu"),
        layers.Dense(32, activation="relu"),
        layers.Dense(1, activation="linear")
    ])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  loss="mse",
                  metrics=[keras.metrics.MeanSquaredError()])

    # è®­ç»ƒï¼ˆä½¿ç”¨æ—©åœï¼‰
    early_stop = keras.callbacks.EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True)
    history = model.fit(
        X_train_scaled, y_train,
        validation_split=0.1,
        epochs=200,
        batch_size=32,
        callbacks=[early_stop],
        verbose=0
    )

    # é¢„æµ‹å¹¶è¯„ä¼°
    y_pred = model.predict(X_test_scaled).reshape(-1)
    mse = mean_squared_error(y_test, y_pred)
    # sklearn çš„ MAPE åœ¨æ–°ç‰ˆæœ¬ä½¿ç”¨ mean_absolute_percentage_error
    try:
        mape = mean_absolute_percentage_error(y_test, y_pred)
    except Exception:
        # fallback: compute manually
        mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1e-8, y_test)))

    # ä¿å­˜æ¨¡å‹ä¸ scaler
    safe_name = "".join(ch if ch.isalnum() else "_" for ch in disc)[:120]
    model.save(os.path.join(OUT_DIR, "models", f"dl_model_{safe_name}.keras"))
    # ä¿å­˜ scaler parameters
    scaler_df = pd.DataFrame({"mean": scaler.mean_, "scale": scaler.scale_}, index=features)
    scaler_df.to_csv(os.path.join(OUT_DIR, f"scaler_{safe_name}.csv"), encoding="utf-8-sig")

    # è®°å½•æ±‡æ€»
    summary_rows.append({
        "Discipline": disc,
        "n_samples": n,
        "train_n": len(X_train),
        "test_n": len(X_test),
        "MSE": float(mse),
        "MAPE": float(mape)
    })

    # ä¿å­˜æµ‹è¯•é¢„æµ‹å¯¹æ¯”
    test_out = test_df[["Institutions", "Discipline", "Cites", "Rank"]].copy()
    test_out["Pred_Rank_DL"] = y_pred
    test_out["Error"] = test_out["Pred_Rank_DL"] - test_out["Rank"]
    fname = os.path.join(OUT_DIR, f"predictions_{safe_name}.csv")
    test_out.to_csv(fname, index=False, encoding="utf-8-sig")
    all_test_results.append(test_out)

    print(f"[{disc}] n={n} train={len(X_train)} test={len(X_test)} MSE={mse:.3f} MAPE={mape:.3f}")

# æ±‡æ€»æ‰€æœ‰å­¦ç§‘è¯„ä¼°
summary_df = pd.DataFrame(summary_rows).sort_values("MSE")
summary_df.to_csv(os.path.join(OUT_DIR, "dl_summary.csv"), index=False, encoding="utf-8-sig")
print("\nâœ… æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°å®Œæˆï¼Œæ±‡æ€»ä¿å­˜ä¸º dl_summary.csv")

# å¯é€‰ï¼šæŠŠæ‰€æœ‰æµ‹è¯•é›†åˆå¹¶ä¿å­˜
if all_test_results:
    pd.concat(all_test_results, ignore_index=True).to_csv(os.path.join(OUT_DIR, "all_test_predictions.csv"),
                                                          index=False, encoding="utf-8-sig")

# ---------------------------
# ç¬¬12é¢˜ï¼šå¯¹ ESI æ•°æ®è¿›è¡Œèšç±»ï¼Œæ‰¾å‡ºä¸åå¸ˆå¤§ç›¸ä¼¼çš„å­¦æ ¡ï¼Œå¹¶åˆ†æåŸå› 
# ---------------------------
print("\n=== ç¬¬12é¢˜ï¼šESI èšç±»ä¸ ECNU ç›¸ä¼¼å­¦æ ¡åˆ†æ ===")

# ä½¿ç”¨åŒä¸€ä»½åŸå§‹ dfï¼ˆæœªæŒ‰å­¦ç§‘åˆ’åˆ†ï¼‰
cluster_df = df.copy()
cluster_features = ["Web of Science Documents", "Cites", "Cites/Paper", "Top Papers"]
cluster_df = cluster_df.dropna(subset=cluster_features).reset_index(drop=True)

# æ ‡å‡†åŒ–å¹¶ KMeans
scaler_cl = StandardScaler()
CF = scaler_cl.fit_transform(cluster_df[cluster_features])

k = 6  # èšç±»æ•°å¯ä»¥è°ƒæ•´
kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED, n_init=20)
cluster_df["Cluster"] = kmeans.fit_predict(CF)

# ä¿å­˜èšç±»åˆ†é…
cluster_df.to_csv(os.path.join(OUT_DIR, "clustered_all.csv"), index=False, encoding="utf-8-sig")

# æ‰¾åˆ° ECNU åœ¨å“ªä¸€ç°‡
target_name = "EAST CHINA NORMAL UNIVERSITY"
ecnu_rows = cluster_df[cluster_df["Institutions"].str.contains(target_name, case=False, na=False)]
if ecnu_rows.empty:
    print("âš ï¸ æ•°æ®ä¸­æœªæ‰¾åˆ° EAST CHINA NORMAL UNIVERSITYï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼å­¦æ ¡æŸ¥æ‰¾ã€‚")
else:
    ecnu_cluster = int(ecnu_rows["Cluster"].iloc[0])
    similars = cluster_df[cluster_df["Cluster"] == ecnu_cluster].copy()
    # å–æŒ‰ Cites æ’åºçš„å‰ 50 ä¸ªä½œä¸ºç›¸ä¼¼å­¦æ ¡åˆ—è¡¨ï¼ˆè‹¥ç°‡å†…æ•°é‡è¾ƒå°‘åˆ™å…¨éƒ¨ï¼‰
    similars_top = similars.sort_values("Cites", ascending=False).head(50)
    out_similar_path = os.path.join(OUT_DIR, "ecnu_cluster_similars.csv")
    similars_top.to_csv(out_similar_path, index=False, encoding="utf-8-sig")
    print(f"ECNU åœ¨ç°‡ {ecnu_cluster}ï¼Œå·²ä¿å­˜ç°‡å†…ç›¸ä¼¼é«˜æ ¡ï¼ˆå‰50ï¼‰åˆ° {out_similar_path}")

    # ç°‡å†…ç‰¹å¾å‡å€¼ï¼Œç”¨äºåˆ†æâ€œä¸ºä»€ä¹ˆè¿™äº›å­¦æ ¡ä¸ ECNU ç±»ä¼¼â€
    cluster_stats = cluster_df.groupby("Cluster")[cluster_features].mean().round(3)
    cluster_stats.to_csv(os.path.join(OUT_DIR, "cluster_analysis.csv"), encoding="utf-8-sig")
    print("å·²ä¿å­˜æ¯ä¸ªç°‡çš„ç‰¹å¾å‡å€¼åˆ° cluster_analysis.csv")

    # ç”Ÿæˆä¸€å¼ ç°‡çš„é›·è¾¾/æ¡å½¢æ¯”è¾ƒå›¾ï¼šECNU vs ç°‡å†…å‡å€¼
    ecnu_profile = cluster_df[cluster_df["Institutions"].str.contains(target_name, case=False, na=False)][cluster_features].mean()
    cluster_mean = cluster_stats.loc[ecnu_cluster]

    # ç»˜å›¾ï¼šæ¡å½¢æ¯”è¾ƒ
    plt.figure(figsize=(8, 5))
    ind = np.arange(len(cluster_features))
    width = 0.35
    plt.bar(ind - width/2, ecnu_profile.values, width, label="ECNU")
    plt.bar(ind + width/2, cluster_mean.values, width, label=f"Cluster {ecnu_cluster} mean")
    plt.xticks(ind, cluster_features, rotation=20)
    plt.ylabel("Standardized / raw scale")
    plt.title("ECNU vs Cluster mean (features)")
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, "ecnu_vs_cluster_mean.png"), dpi=300)
    print("å·²ç”Ÿæˆå›¾ ecnu_vs_cluster_mean.png ä¾›ç›´è§‚æ¯”è¾ƒ")

# ---------------------------
# å°ç»“è¾“å‡º
# ---------------------------
print("\n--- è¿è¡Œå®Œæˆ ---")
print("è¾“å‡ºç›®å½•:", OUT_DIR)
print("åŒ…å«æ–‡ä»¶æ ·ä¾‹ï¼š", os.listdir(OUT_DIR)[:20])
print("è¯·æŸ¥çœ‹ dl_summary.csvï¼ˆæŒ‰å­¦ç§‘ MSE / MAPEï¼‰ï¼Œä»¥åŠ ecnu_cluster_similars.csvï¼ˆä¸ ECNU åŒç°‡é«˜æ ¡ï¼‰")
